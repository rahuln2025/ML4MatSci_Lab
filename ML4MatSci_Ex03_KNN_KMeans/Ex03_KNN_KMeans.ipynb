{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7457d8c4",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbours and K-Means Clustering\n",
    "\n",
    "In this exercise, we shall use k-nearest neighbours (KNN) and k-means clustering for classficiation of digits from the MNIST handwritten dataset. \n",
    "\n",
    "KNN is a supervised learning method, whereas k-means clustering is an unsupervised learning method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f39c314c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: threadpoolctl==3.1.0 in c:\\users\\narkh\\anaconda3\\lib\\site-packages (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\narkh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\narkh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\narkh\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow-intel (c:\\users\\narkh\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "#optional if the following error arises with sklearn implemenations:\n",
    "#AttributeError: 'NoneType' object has no attribute 'split'\n",
    "# refer the link: \n",
    "# https://stackoverflow.com/questions/71352354/sklearn-kmeans-is-not-working-as-i-only-get-nonetype-object-has-no-attribute\n",
    "\n",
    "! pip install threadpoolctl==3.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef3dda4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from ipywidgets import interact\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ee57bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will not work until a Python script named 'MyPCA.py' is created\n",
    "# which contains the PCA class from the exercise 02.\n",
    "\n",
    "# If such a file does not exist, use sklearn's PCA implementation as disccused in exercise 02\n",
    "\n",
    "#import the PCA class implemention\n",
    "from MyPCA import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dcab4ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'> (70000, 784)\n",
      "<class 'pandas.core.series.Series'> (70000,)\n",
      "<class 'numpy.ndarray'> (784,)\n",
      "<class 'str'> 5\n"
     ]
    }
   ],
   "source": [
    "# import MNIST dataset (might take a few seconds)\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "images, labels = fetch_openml('mnist_784', version=1, return_X_y=True, parser = 'auto')\n",
    "\n",
    "# check the datatype and shape of the data\n",
    "print(type(images), images.shape)\n",
    "print(type(labels), labels.shape)\n",
    "\n",
    "# if not in NumPy, convert the data to NumPy array\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# check the shape of an individual image sample\n",
    "print(type(images[0]), images[0].shape)\n",
    "print(type(labels[0]), labels[0])\n",
    "\n",
    "# select 2000 samples from the whole MNIST dataset\n",
    "num_datapoints = 2000\n",
    "# some preprocessing\n",
    "X = (images.reshape(-1, 28*28)[:num_datapoints])/ 255 \n",
    "# dividing by 255 will scale pixel values between [0, 1]\n",
    "y = labels.astype(float) #convert string labels to float\n",
    "y = y[:num_datapoints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d64b93a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  0.0 | count:  191\n",
      "Label:  1.0 | count:  220\n",
      "Label:  2.0 | count:  198\n",
      "Label:  3.0 | count:  191\n",
      "Label:  4.0 | count:  214\n",
      "Label:  5.0 | count:  180\n",
      "Label:  6.0 | count:  200\n",
      "Label:  7.0 | count:  224\n",
      "Label:  8.0 | count:  172\n",
      "Label:  9.0 | count:  210\n",
      "total samples:  2000\n"
     ]
    }
   ],
   "source": [
    "# Show counts of samples for each digit among the 2000 samples\n",
    "\n",
    "labels_unique = np.unique(y)\n",
    "counts = []\n",
    "for label in labels_unique:\n",
    "    count = np.sum(y == label)\n",
    "    counts.append(count)\n",
    "    print(\"Label: \", label, \"| count: \", count)\n",
    "\n",
    "print(\"total samples: \", sum(counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a469ad1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot an image\n",
    "def image_plotter(X, labels, ix):\n",
    "    plt.figure(figsize = (4, 4))\n",
    "    plt.imshow(X[ix].reshape(28, 28), cmap = 'gray')\n",
    "    plt.title('Label: '+ str(labels[ix]))\n",
    "    \n",
    "image_plotter(X, y, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa798b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_digits(X, y, classes, samples_per_class):\n",
    "    \"\"\"\n",
    "    Extract specified number of samples for each given digit from the MNIST dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - X (np.array): Array of images from MNIST dataset.\n",
    "    - y (np.array): Corresponding labels for the MNIST dataset images.\n",
    "    - classes (list of int): List of digits to extract (e.g., [0, 1, 2]).\n",
    "    - samples_per_class (int): Number of samples to extract for each class.\n",
    "\n",
    "    Returns:\n",
    "    - tuple of np.array: Tuple containing the extracted features matrix and labels vector.\n",
    "    \"\"\"\n",
    "    X_extracted = []\n",
    "    y_extracted = []\n",
    "    \n",
    "    for digit in classes:\n",
    "        # Find the indices where the label equals the current digit\n",
    "        indices = np.where(y == digit)[0]\n",
    "        \n",
    "        # Check if there are enough samples available\n",
    "        if len(indices) < samples_per_class:\n",
    "            raise ValueError(f\"Not enough samples for digit {digit}. Only {len(indices)} available.\")\n",
    "        \n",
    "        # Extract the samples for this digit\n",
    "        selected_indices = indices[:samples_per_class]\n",
    "        X_extracted.append(X[selected_indices])\n",
    "        y_extracted.append(y[selected_indices])\n",
    "    \n",
    "    # Concatenate lists into arrays\n",
    "    X_extracted = np.concatenate(X_extracted, axis=0)\n",
    "    y_extracted = np.concatenate(y_extracted, axis=0)\n",
    "    \n",
    "    return X_extracted, y_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56e5fff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 784) (300,)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "classes = [1, 7, 3]  # Digits to extract\n",
    "samples_per_class = 100 # Number of samples per digit\n",
    "\n",
    "X_classes, y_classes = extract_digits(X, y, classes, samples_per_class)\n",
    "print(X_classes.shape, y_classes.shape)  # Check the shape of the extracted arrays\n",
    "print(y_classes[y_classes == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ff82b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index:  285 Label:  3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a random image\n",
    "ix_l = np.random.randint(0, len(X_classes))\n",
    "print(\"index: \", ix_l, \"Label: \", y_classes[ix_l])\n",
    "image_plotter(X_classes, y_classes, ix_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c384dc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot scatter plot of all the samples in dataset with labels\n",
    "\n",
    "def scatter_plot(colors_labels, X, y, title = 'Plot of samples'):\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Set to keep track of which labels have been added to the legend\n",
    "    legend_labels = set()\n",
    "\n",
    "    for point, label in zip(X, y):\n",
    "        if label not in legend_labels:\n",
    "            ax.scatter(point[0], point[1], c=colors[label], label=label)\n",
    "            legend_labels.add(label)  # Mark this label as added\n",
    "        else:\n",
    "            ax.scatter(point[0], point[1], c=colors[label])\n",
    "\n",
    "    # Add a legend, labels, and title\n",
    "    ax.legend(title='Label')\n",
    "    plt.xlabel('Principal component 1')\n",
    "    plt.ylabel('Principal component 2')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85b6fa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(201, 784) (99, 784)\n"
     ]
    }
   ],
   "source": [
    "# create train and test datasets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_classes, y_classes, test_size=0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0963e9aa",
   "metadata": {},
   "source": [
    "## k-nearest neighbours classification (supervised learning)\n",
    "\n",
    "The k-nearest neighbors (k-NN) classification algorithm is a type of instance-based learning, where the function is only approximated locally and all computation is deferred until classification.\n",
    "\n",
    "Given a new data point $x$, the algorithm finds the $k$ training examples that are closest to it in terms of distance, and then predicts the output based on the outputs of these $k$ neighbors. Let $N_k(x)$ be the set of $k$ nearest neighbors of $x$, and let $y_i$ be the output of the $i$-th nearest neighbor. The predicted output $\\hat{y}$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\text{mode}\\{y_i : x_i \\in N_k(x)\\}\n",
    "$$\n",
    "\n",
    "where $\\text{mode}\\{.\\}$ is the mode function that returns the most common value among its arguments.\n",
    "\n",
    "The distance between two data points can be measured using various distance metrics, such as Euclidean distance, Manhattan distance, or Minkowski distance. The choice of distance metric depends on the problem at hand.\n",
    "\n",
    "In summary, k-nearest neighbors classification works by finding the $k$ nearest neighbors of a new data point and predicting the output based on the most common output among these neighbors.\n",
    "\n",
    "\n",
    "Some definitions:\n",
    "\n",
    "1. *Instance based learning*: it makes predictions based on the specific instances of the training data, rather than constructing a general model that captures the underlying relationship between the input and output variables.\n",
    "\n",
    "2. *Function is only approximated locally*: the prediction for a new data point is based on the outputs of its k nearest neighbors in the training data. This is in contrast to other machine learning algorithms, such as linear regression or decision trees, where a global model is constructed to approximate the function over the entire input space.\n",
    "\n",
    "### Illustration\n",
    "\n",
    "![KNN_Illustration](KNN_Illustration.png)\n",
    "\n",
    "\n",
    "### Algorithm:\n",
    "\n",
    "1) Select the number of k neighbors.\n",
    "\n",
    "2) Calculate Euclidean distance of each sample w.r.t all other samples of the dataset.\n",
    "\n",
    "3) For each sample, sort the distances in ascending order, and select the k nearest neighbors. \n",
    "\n",
    "4) Find the most frequent label amongst the k nearest neighbors to each sample. Assign that as the label of that sample. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b1bc4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.base import BaseEstimator\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def most_common(lst):\n",
    "    \"\"\"\n",
    "    Find the mode of the labels.\n",
    "    Args:\n",
    "    - lst: list of labels.\n",
    "    \n",
    "    Returns:\n",
    "    - The most frequent label.\n",
    "    \"\"\"\n",
    "    #return #max(set(lst), key=lst.count)\n",
    "    return mode(lst).mode[0]\n",
    "\n",
    "def euclidean(point, data):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between a point and all points in a dataset.\n",
    "    Args:\n",
    "    - point: A numpy array representing the test sample.\n",
    "    - data: A numpy array representing the training samples.\n",
    "    \n",
    "    Returns:\n",
    "    - distances: Numpy array of distances.\n",
    "    \"\"\"\n",
    "    #TASK: Compute the euclidean distance between a point and all points of the dataset\n",
    "    distances = None\n",
    "    \n",
    "    \n",
    "    return distances\n",
    "\n",
    "class My_KNeighborsClassifier(BaseEstimator):\n",
    "    def __init__(self, k=5, dist_metric=euclidean):\n",
    "        \"\"\"\n",
    "        Initialize the KNeighborsClassifier.\n",
    "        Args:\n",
    "        - k: Number of neighbors to consider.\n",
    "        - dist_metric: Function to compute the distance.\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.dist_metric = dist_metric\n",
    "        self.is_fitted_ = False  # Attribute to track if the estimator is fitted\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Fit the KNeighbors classifier with the training data.\n",
    "        Args:\n",
    "        - X_train: Training data features.\n",
    "        - y_train: Training data labels.\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.is_fitted_ = True  # Indicate that fitting is complete\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the class labels for the test set.\n",
    "        Args:\n",
    "        - X_test: Test data features.\n",
    "        \n",
    "        Returns:\n",
    "        - predictions: Numpy array of predicted labels.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'is_fitted_')\n",
    "        \n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            # TASK: Compute distances from x to all training samples\n",
    "            distances = None  # Replace None with your code using self.dist_metric\n",
    "\n",
    "            # TASK: Sort the training labels by distance and select the k nearest\n",
    "            k_nearest_labels = None  # Replace None with your code to sort and select k nearest\n",
    "\n",
    "            # TASK: Find the most common label among the k nearest labels\n",
    "            predicted_label = None  # Replace None with your code using most_common\n",
    "\n",
    "            predictions.append(predicted_label)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Evaluate the accuracy of the classifier.\n",
    "        Args:\n",
    "        - X_test: Test data features.\n",
    "        - y_test: Test data labels.\n",
    "        \n",
    "        Returns:\n",
    "        - accuracy: The accuracy of the classifier on the test data.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X_test)\n",
    "        accuracy = None  # TASK: Compute the accuracy of the predictions\n",
    "        return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5aead1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires MyPCA class implementation\n",
    "\n",
    "# Preprocess data\n",
    "mypca = MyPCA(2)\n",
    "mypca.fit(X_train)\n",
    "X_train_reduced = mypca.transform(X_train)\n",
    "X_test_reduced = mypca.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a00f3a2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'round'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-13f0563c0e9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0my_test_pred_my_knn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_reduced\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtest_accuracy_my_knn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmy_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_reduced\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test accuracy my knn: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy_my_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'round'"
     ]
    }
   ],
   "source": [
    "# WILL NOT RUN UNTIL THE CODE BLOCK ABOVE IS COMPLETED\n",
    "\n",
    "# Use the KNeighborsClassifier class \n",
    "\n",
    "my_knn = My_KNeighborsClassifier(k=5)\n",
    "my_knn.fit(X_train_reduced, y_train)\n",
    "y_test_pred_my_knn = my_knn.predict(X_test_reduced)\n",
    "test_accuracy_my_knn = my_knn.evaluate(X_test_reduced, y_test)\n",
    "print(\"Test accuracy my knn: \", test_accuracy_my_knn.round(4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4592a3",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours with Scikit Learn\n",
    "\n",
    "To know more about the KNeighborsClassifier from sklearn, refer to the documentation link below. They also have a number of examples which explain different concepts. \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6438b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as sk_KNeighborsClassifier\n",
    "\n",
    "sk_knn = sk_KNeighborsClassifier(n_neighbors = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fdf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_knn.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d20608",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_sk_knn = sk_knn.predict(X_test_reduced)\n",
    "sk_knn_accuracy = np.mean(y_test_pred_sk_knn == y_test)\n",
    "print(\"Test accuracy sk learn KNN: \", sk_knn_accuracy.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b127b907",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {7: 'blue', 1: 'green', 3: 'orange', 5: 'yellow'}\n",
    "scatter_plot(colors, X_test_reduced, y_test, title = 'Plot of true labels of test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd741e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "def plot_decision_boundary(estimator, X, y, title = 'Decision boundary', key = None):\n",
    "    \n",
    "     # Plotting the decision boundary\n",
    "    disp = DecisionBoundaryDisplay.from_estimator(\n",
    "           estimator, X, response_method = 'predict',\n",
    "           xlabel = 'PC 1', ylabel = 'PC 2', alpha = 0.5)\n",
    "\n",
    "    # Prepare to track labels and assign colors\n",
    "    unique_labels = np.unique(y)\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(unique_labels)))  # Use a colormap\n",
    "    label_color_dict = dict(zip(unique_labels, colors))\n",
    "\n",
    "    # Set to keep track of which labels have been added to the legend\n",
    "    legend_labels = set()\n",
    "\n",
    "    # Plot each point, ensuring each label is added to the legend only once\n",
    "    for point, label in zip(X, y):\n",
    "       # if key is not None:\n",
    "          #  label = key[label]\n",
    "        if label not in legend_labels:\n",
    "            disp.ax_.scatter(point[0], point[1], color=label_color_dict[label], label=label, edgecolor='k')\n",
    "            legend_labels.add(label)  # Mark this label as added\n",
    "        else:\n",
    "            disp.ax_.scatter(point[0], point[1], color=label_color_dict[label], edgecolor='k')\n",
    "\n",
    "    # Add a legend, title and show plot\n",
    "    plt.legend(title='Label')\n",
    "    if key is not None:\n",
    "        plt.title(title + '\\n' + str(key))\n",
    "    else:\n",
    "        plt.title(title)\n",
    "    plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3060906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(sk_knn, X_test_reduced, y_test, title = 'Decision boundary of sklearn KNN on test data', key = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acd18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WILL NOT RUN UNTIL KNeighborsClassifier class is complete\n",
    "\n",
    "plot_decision_boundary(my_knn, X_test_reduced, y_test, title = 'Decision boundary of my KNN on test data', key = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934cdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119c363e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "983f8469",
   "metadata": {},
   "source": [
    "## K-means clustering\n",
    "\n",
    "### Unsupervised learning\n",
    "\n",
    "Unsupervised learning, also known as unsupervised machine learning, uses machine learning algorithms to analyze and cluster unlabeled datasets. These algorithms discover hidden patterns or data groupings without the need for human intervention. Its ability to discover similarities and differences in information make it the ideal solution for exploratory data analysis, cross-selling strategies, customer segmentation, and image recognition. \n",
    "\n",
    "[source](https://www.ibm.com/topics/unsupervised-learning)\n",
    "\n",
    "There is no specific $y$ variable for labels, hence there is no clear ***correct*** answer.\n",
    "\n",
    "We have to now perform a similar classification of the samples corresponding to the selected three digits from the MNIST dataset, but without providing the labels to the samples in model fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184fac7b",
   "metadata": {},
   "source": [
    "### How would you go about solving this problem?\n",
    "\n",
    "The k-means clustering algorithm works by iteratively performing 2 steps:\n",
    "\n",
    "Step 1. assigning each data point to one of k clusters based on the distance between the data point and the cluster’s centroid\n",
    "\n",
    "Step 2. updating the centroid of each cluster based on the mean of the data points assigned to it.\n",
    "\n",
    "#### Defining it more clearly\n",
    "\n",
    "Let $x_1, x_2, ..., x_n$ be the $n$ data points, and let $c_1, c_2, ..., c_k$ be the centroids of the $k$ clusters. The objective of k-means clustering is to minimize the within-cluster sum of squares (WCSS), which is given by:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^k \\sum_{x \\in c_i} ||x - \\mu_i||^2\n",
    "$$\n",
    "\n",
    "where $\\mu_i$ is the mean of the data points in cluster $c_i$.\n",
    "\n",
    "The k-means clustering algorithm works by iteratively assigning each data point to the cluster with the nearest centroid and then updating the centroid of each cluster based on the mean of the data points assigned to it. The algorithm terminates when the assignments no longer change.\n",
    "\n",
    "The assignment step can be expressed mathematically as:\n",
    "\n",
    "$$\n",
    "c_i = \\{x : ||x - \\mu_i|| \\leq ||x - \\mu_j|| \\text{ for all } 1 \\leq j \\leq k\\}\n",
    "$$\n",
    "\n",
    "The update step can be expressed mathematically as:\n",
    "\n",
    "$$\n",
    "\\mu_i = \\frac{1}{|c_i|} \\sum_{x \\in c_i} x\n",
    "$$\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "1) Choose the number of clusters k\n",
    "\n",
    "2) Initialize the centroids for each cluster\n",
    "\n",
    "3) Assign each point to the closest cluster centroid\n",
    "\n",
    "4) Compute the two centroids of newly formed clusters\n",
    "\n",
    "5) Repeat the previous two steps, until there is no change in the centroid positions\n",
    "\n",
    "### Illustration\n",
    "\n",
    "![KMeans_GIF](K-means_convergence.gif)\n",
    "\n",
    "\n",
    "\n",
    "##### Note: KMeans clustering predicts the clusters to which the samples belong. Hence, their outputs are not the labels itself and need to be mapped to the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52432d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_KMeans:\n",
    "    def __init__(self, n_clusters, centroids=None, max_iter=300, tol=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the KMeans clustering algorithm.\n",
    "        Args:\n",
    "        - n_clusters: The number of clusters to form.\n",
    "        - centroids: Initial position of centroids, if provided.\n",
    "        - max_iter: Maximum number of iterations of the KMeans algorithm for a single run.\n",
    "        - tol: Tolerance to declare convergence.\n",
    "        \"\"\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.centroids = centroids\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Compute KMeans clustering.\n",
    "        Args:\n",
    "        - X: ndarray of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize centroids for clusters\n",
    "        if self.centroids is None:\n",
    "            # TASK: Randomly initialize the centroids as k random samples from X\n",
    "            random_indices = None  # Replace None with your code\n",
    "            self.centroids = None  # Replace None with your code to select centroids using random_indices\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            # Assign clusters based on the closest centroid\n",
    "            distances = self._compute_distances(X)\n",
    "            self.labels_ = None  # Replace None with your code to assign each sample to the nearest centroid\n",
    "\n",
    "            # Compute new centroids\n",
    "            new_centroids = None  # Replace None with your code to compute new centroids as the mean of points in each cluster\n",
    "\n",
    "            # Check for convergence\n",
    "            if None:  # Replace None with your convergence checking code\n",
    "                break\n",
    "            \n",
    "            # Update centroids\n",
    "            self.centroids = new_centroids\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the closest cluster each sample in X belongs to.\n",
    "        Args:\n",
    "        - X: ndarray of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        - labels: Index of the cluster each sample belongs to.\n",
    "        \"\"\"\n",
    "        distances = self._compute_distances(X)\n",
    "        return None  # Replace None with your code to return cluster indices based on nearest centroids\n",
    "\n",
    "    def _compute_distances(self, X):\n",
    "        \"\"\"\n",
    "        Compute the distance between each point in X and the centroids.\n",
    "        Args:\n",
    "        - X: ndarray of shape (n_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        - distances: Array of distances of each sample to each centroid.\n",
    "        \"\"\"\n",
    "        return None  # Replace None with your code to calculate distances from each sample to each centroid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d586125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_kmeans_labels_to_original(kmeans_labels, original_labels):\n",
    "    \"\"\"\n",
    "    Map KMeans cluster labels to the most common original label in each cluster.\n",
    "\n",
    "    Parameters:\n",
    "    - kmeans_labels (np.array): Array of labels generated by KMeans.\n",
    "    - original_labels (np.array): Array of original labels for the data.\n",
    "\n",
    "    Returns:\n",
    "    - np.array: Array of original labels mapped from KMeans labels.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store the mapping from KMeans labels to original labels\n",
    "    label_mapping = {}\n",
    "\n",
    "    # Identify the most common original label in each cluster\n",
    "    for cluster_label in np.unique(kmeans_labels):\n",
    "        # Find indices where the cluster label matches\n",
    "        indices = np.where(kmeans_labels == cluster_label)\n",
    "        # Extract original labels corresponding to these indices\n",
    "        original_labels_in_cluster = original_labels[indices]\n",
    "        # Find the most frequent label\n",
    "        values, counts = np.unique(original_labels_in_cluster, return_counts=True)\n",
    "        most_common_label = values[np.argmax(counts)]\n",
    "        # Map the cluster label to the most common original label\n",
    "        label_mapping[cluster_label] = most_common_label\n",
    "    \n",
    "    \n",
    "    # Apply the mapping to the KMeans labels to create a mapped label array\n",
    "    mapped_labels = np.array([label_mapping[label] for label in kmeans_labels])\n",
    "\n",
    "    return mapped_labels, label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3985b5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WILL NOT RUN UNTIL KMeans CLASS IS COMPLETED\n",
    "\n",
    "my_kmeans = My_KMeans(n_clusters = 3)\n",
    "my_kmeans.fit(X_train_reduced)\n",
    "\n",
    "clusters_test_my = my_kmeans.predict(X_test_reduced)\n",
    "y_test_pred_my_kmeans, _ = map_kmeans_labels_to_original(clusters_test_my, y_test)\n",
    "\n",
    "test_accuracy_my_kmeans = np.mean(y_test_pred_my_kmeans == y_test)\n",
    "print(\"Test accuracy with my KMeans: \", test_accuracy_my_kmeans.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12286d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# WILL NOT RUN UNTIL KMeans CLASS IS COMPLETED\n",
    "plot_decision_boundary(my_kmeans, X_test_reduced, y_test_clusters, title = 'Decision boundary for my implemented of KMeans clustering', key = key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160dbcec",
   "metadata": {},
   "source": [
    "## K-Means Clustering with Scikit Learn\n",
    "\n",
    "Lets see the sklearn approach of using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a826b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we shall use the data with reduced dimensionality to two principal components\n",
    "\n",
    "from sklearn.cluster import KMeans as sk_KMeans\n",
    "\n",
    "sk_kmeans = sk_KMeans(n_clusters = 3, random_state = 0, n_init = \"auto\", max_iter=300, tol=1e-6)\n",
    "sk_kmeans.fit(X_train_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae06bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_test_sk = sk_kmeans.predict(X_test_reduced)\n",
    "y_test_pred_sk_kmeans, key = map_kmeans_labels_to_original(clusters_test_sk, y_test)\n",
    "\n",
    "test_accuracy_sk_kmeans = np.mean(y_test_pred_sk_kmeans == y_test)\n",
    "print(\"Test accuracy with sklearn KMeans: \", test_accuracy_sk_kmeans.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7553181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WILL NOT RUN UNTIL KMeans CLASS IS COMPLETED\n",
    "\n",
    "print(\"No of misclassficiations of my KMeans: \", np.sum(y_test_pred_my_kmeans != y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71f07d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"No of misclassficiations of sklearn KMeans: \", np.sum(y_test_pred_sk_kmeans != y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ebce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step is necessary for plotting the decision boundary of KMeans clustering correctly\n",
    "# Convert test labels to cluster indices \n",
    "# Here we convert the labels of y_test dataset into corresponding cluster indices\n",
    "\n",
    "_, key = map_kmeans_labels_to_original(clusters_test_sk, y_test)\n",
    "print(key) # Key is a dictionary whose keys are the cluster indices and the values are the corresponding labels from y_test\n",
    "\n",
    "y_test_clusters = y_test.copy()\n",
    "\n",
    "for k, v in key.items():\n",
    "    cluster = k\n",
    "    label = v\n",
    "    y_test_clusters[y_test == label] = cluster\n",
    "    \n",
    "#print(y_test_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58576ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WILL NOT RUN UNTIL KMeans CLASS IS COMPLETED\n",
    "\n",
    "plot_decision_boundary(my_kmeans, X_test_reduced, y_test_clusters, title = 'Decision boundary for my implemented of KMeans clustering', key = key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da91781",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_boundary(sk_kmeans, X_test_reduced, y_test_clusters, title = 'Decision boundary for my sklearn KMeans clustering', key = key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c11308",
   "metadata": {},
   "source": [
    "### REMARK: KMeans clustering leads to formation of Voronoi cells around the respective cluster centroids. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d037e615",
   "metadata": {},
   "source": [
    "## Illustration: Impact of scaling data \n",
    "\n",
    "For distance based methods such as K-Nearest Neighbors and KMeans Clustering, scaling the data before applying the method is necessary. The code blocks below show the effect of scaling the data. \n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "Consider two features in a dataset, $A$ and $B$. Suppose $A$ ranges from 0 to 1000, and $B$ ranges from 0 to 1. The Euclidean distance $d$ between two points $x = (x_A, x_B)$ and $y = (y_A, y_B)$ is calculated as:\n",
    "\n",
    "$$\n",
    "dist(x, y) = \\sqrt{(x_A - y_A)^2 + (x_B - y_B)^2}\n",
    "$$\n",
    "\n",
    "Here, the larger scale of $A$ will dominate the distance calculation, making changes in $B$ almost irrelevant, even if they are crucial for clustering or classification. This demonstrates the need for feature scaling.\n",
    "\n",
    "The image below shows the effect on K-Nearest Neighbors method if scaling is not done. \n",
    "\n",
    "![scaling_impact](scaling_importance.png)\n",
    "\n",
    "The detailed code for this example can be found here: https://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21a9c83",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "### How to decide the hyperparameters here ? :\n",
    "\n",
    "1) Number of neighbours in knn.\n",
    "\n",
    "2) Number of clusters in kmeans.\n",
    "\n",
    "3) Appropriate number of principal components.\n",
    "\n",
    "Let us consider the case of KNN, where we have to decide the best set of parameter pairs of number of principal components and number of nearest neighbors. \n",
    "\n",
    "Approach: \n",
    "\n",
    "### Cross-Validation\n",
    "\n",
    "![kfold_validation.png](kfold_validation.png)\n",
    "\n",
    "\n",
    "### Grid Search\n",
    "\n",
    "Grid search exhaustively generates all the possible (hyper-)parameter combinations from the given set of parameter values. Then it trains the model with all these combinaitions of hyperparameter, computes the performance metric and returns the parameters that lead to the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aec089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 7],\n",
    "    'n_components': [10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "param_combos = []\n",
    "    \n",
    "for n_neighbor in param_grid['n_neighbors']:\n",
    "    for n_component in param_grid['n_components']:\n",
    "        \n",
    "        param_combo = {\n",
    "            'n_neighbors': n_neighbor,\n",
    "            'n_components': n_component\n",
    "        }\n",
    "        param_combos.append(param_combo)\n",
    "        \n",
    "print(param_combos, len(param_combos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "692ccacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_split(X, y, k=5):\n",
    "    \"\"\"Splits data into k folds for cross-validation\"\"\"\n",
    "    fold_size = len(X) // k\n",
    "    indices = np.arange(len(X))\n",
    "    #np.random.shuffle(indices)\n",
    "    \n",
    "    folds = []\n",
    "    for i in range(k):\n",
    "        val_indices = indices[i * fold_size: (i + 1) * fold_size]\n",
    "        train_indices = np.setdiff1d(indices, val_indices)\n",
    "        folds.append((train_indices, val_indices))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def grid_search_cv(X, y, param_grid, k_folds = 5, model = 'knnWith ', centroids = None):\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    param_combinations = []\n",
    "    \n",
    "    if model == 'knn':\n",
    "    \n",
    "        for n_neighbor in param_grid['n_neighbors']:\n",
    "            for n_component in param_grid['n_components']:\n",
    "        \n",
    "                param_combo = {\n",
    "                    'n_neighbors': n_neighbor,\n",
    "                    'n_components': n_component\n",
    "                }\n",
    "                param_combinations.append(param_combo)\n",
    "                \n",
    "                \n",
    "    elif model == 'kmeans':\n",
    "        \n",
    "        for n_clusters in param_grid['n_clusters']:\n",
    "            for n_component in param_grid['n_components']:\n",
    "                \n",
    "                param_combo = {\n",
    "                    'n_clusters': n_clusters,\n",
    "                    'n_components': n_component\n",
    "                }\n",
    "                param_combinations.append(param_combo)                \n",
    "    \n",
    "    folds = k_fold_split(X, y, k = k_folds)\n",
    "    \n",
    "    with tqdm(total=len(param_combinations) * k_folds, desc='Grid Search') as pbar:\n",
    "        for params in param_combinations: \n",
    "            cv_scores = []\n",
    "        \n",
    "            for train_indices, val_indices in folds: \n",
    "                X_train, X_val = X[train_indices], X[val_indices]\n",
    "                y_train, y_val = y[train_indices], y[val_indices]\n",
    "            \n",
    "                #apply PCA\n",
    "                mypca = MyPCA(params['n_components'])\n",
    "                mypca.fit(X_train)\n",
    "                X_train_pca = mypca.transform(X_train)\n",
    "                X_val_pca = mypca.transform(X_val)\n",
    "            \n",
    "                #train with KNN\n",
    "                if model == 'knn':\n",
    "                \n",
    "                    \n",
    "                    # uncomment and replace after completing class implementation of KNN\n",
    "                    #knn = My_KNeighborsClassifier(k = params['n_neighbors'])\n",
    "        \n",
    "                    knn = sk_KNeighborsClassifier(n_neighbors = params['n_neighbors'])\n",
    "                    knn.fit(X_train_pca, y_train)\n",
    "            \n",
    "                    #validate model\n",
    "                    y_val_pred = knn.predict(X_val_pca)\n",
    "                    accuracy = np.mean(y_val_pred == y_val)\n",
    "                    cv_scores.append(accuracy)\n",
    "                \n",
    "                elif model == 'kmeans':\n",
    "                    \n",
    "                    # uncomment and replace after completing class implementation of KMeans Clustering\n",
    "                    #kmeans = My_KMeans(n_clusters = params['n_clusters'])\n",
    "                    \n",
    "                    kmeans = sk_KMeans(n_clusters = params['n_clusters'], random_state = 0, n_init = \"auto\", max_iter = 300, tol = 1e-6)\n",
    "                    kmeans.fit(X_train_pca)\n",
    "                    \n",
    "                    #validate model\n",
    "                    y_val_pred = kmeans.predict(X_val_pca)\n",
    "                    accuracy = np.mean(y_val_pred == y_val)\n",
    "                    cv_scores.append(accuracy)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "            mean_cv_score = np.mean(cv_scores)\n",
    "        \n",
    "            if mean_cv_score > best_score:\n",
    "                best_score = mean_cv_score\n",
    "                best_params = params\n",
    "    \n",
    "    return best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d14ed94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search:   0%|                                                                              | 0/60 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sk_KNeighborsClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-1cef9388f838>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mbest_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_folds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'knn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best Parameters: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-8409f926b522>\u001b[0m in \u001b[0;36mgrid_search_cv\u001b[1;34m(X, y, param_grid, k_folds, model, centroids)\u001b[0m\n\u001b[0;32m     69\u001b[0m                     \u001b[1;31m#knn = My_KNeighborsClassifier(k = params['n_neighbors'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m                     \u001b[0mknn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msk_KNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_neighbors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m                     \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sk_KNeighborsClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "# KNN hyperparameters: number of principal components, number of nearest neighbours\n",
    "\n",
    "# Example parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': [5, 7, 9, 11],\n",
    "    'n_components': [10, 20, 30]\n",
    "}\n",
    "\n",
    "best_params, best_score = grid_search_cv(X_train, y_train, param_grid, k_folds = 5, model = 'knn')\n",
    "\n",
    "print(\"Best Parameters: \", best_params)\n",
    "print(\"Best Cross-Validation Accuracy: {:.4f}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dee5e732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params accuracy for my KNN implementation:  0.9596\n"
     ]
    }
   ],
   "source": [
    "# WILL NOT WORK UNTIL My_KNeighborsClassifier CLASS IS COMPLETED\n",
    "\n",
    "my_pca_best = MyPCA(20)\n",
    "my_pca_best.fit(X_train)\n",
    "X_train_reduced = my_pca_best.transform(X_train)\n",
    "X_test_reduced = my_pca_best.transform(X_test)\n",
    "\n",
    "#my_knn_best = My_KNeighborsClassifier(k = 5)\n",
    "my_knn_best = sk_KNeighborsClassifier(n_neighbors = 5)\n",
    "my_knn_best.fit(X_train_reduced, y_train)\n",
    "\n",
    "y_test_knn_best_pred = my_knn_best.predict(X_test_reduced)\n",
    "best_knn_accuracy = np.mean(y_test_knn_best_pred == y_test)\n",
    "\n",
    "print(\"Best params accuracy for my KNN implementation: \", best_knn_accuracy.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a41bd31",
   "metadata": {},
   "source": [
    "### Tasks with sklearn: \n",
    "\n",
    "- Implement PCA using sklearn tools.\n",
    "\n",
    "- Implement KNeighborClassifier from sklearn tools.\n",
    "\n",
    "- Implement Grid Search and Cross-Validation using sklearn tools.\n",
    "\n",
    "- Use this for finding the best parameters for K-Nearest Neighbors, i.e. the number of principal components and the number of nearest neighbors. Use the following paramter ranges: \n",
    "    \n",
    "    ```Python\n",
    "    param_grid = {\n",
    "    'n_neighbors': range(1, 30),\n",
    "    'n_components': range(2, 50)\n",
    "     }\n",
    "    ```\n",
    "- How can you find the second best set of parameters?\n",
    "\n",
    "- In case of a large number of parameter combinations, is there an efficient aleternative to Grid Search in the sklearn tools? \n",
    "\n",
    "- Implement all the steps, i.e. PCA, KNeighborsClassifer, Grid Search with Cross Validation using a Pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d7dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a8fe89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002f8c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b42ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
